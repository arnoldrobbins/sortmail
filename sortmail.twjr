\input texinfo   @c -*-texinfo-*-
@c vim: filetype=texinfo tabstop=4 shiftwidth=4
@c %**start of header (This is for running Texinfo on a region.)
@setfilename sortmail.info
@settitle Sortmail
@c %**end of header (This is for running Texinfo on a region.)

@c Change how xref titles are quoted.
@dquotexrefs
@c And let braces in index entries work.
@allowindexbraces
@ifclear FORPRINT
@pdflinkcolor
@urllinkcolor
@hideurls
@end ifclear

@c The following information should be updated here only!
@c This sets the edition of the document.

@c These apply across the board.
@set UPDATE-MONTH November, 2022
@set EDITION 1.11

@set TITLE Sortmail: A Program To Sort Mailboxes
@set SHORTTITLE Sortmail

@iftex
@set DOCUMENT book
@set CHAPTER chapter
@set APPENDIX appendix
@set SECTION section
@set SUBSECTION subsection
@end iftex
@ifhtml
@set DOCUMENT Web page
@set CHAPTER chapter
@set APPENDIX appendix
@set SECTION section
@set SUBSECTION subsection
@end ifhtml
@ifinfo
@set DOCUMENT Info file
@set CHAPTER major node
@set APPENDIX major node
@set SECTION minor node
@set SUBSECTION node
@end ifinfo
@ifdocbook
@set DOCUMENT book
@set CHAPTER chapter
@set APPENDIX appendix
@set SECTION section
@set SUBSECTION subsection
@end ifdocbook

@c some special symbols
@ifnottex
@macro ii{text}
@i{\text\}
@end macro
@end ifnottex

@c merge the function and variable indexes into the concept index
@c do so without the code font, and in the index entries do the
@c font management ourselves.  Also merge in the chunk definition
@c and reference entries, which jrweave creates for us.
@ifnothtml
@synindex fn cp
@synindex vr cp
@synindex cd cp
@synindex cr cp
@end ifnothtml

@c If "finalout" is commented out, the printed output will show
@c black boxes that mark lines that are too long.  Thus, it is
@c unwise to comment it out when running a master in case there are
@c overfulls which are deemed okay.

@iftex
@c @finalout
@end iftex

@copying
@docbook
<para>Published by:</para>

<literallayout class="normal">Arnold David Robbins
P.O.B. 354
Nof Ayalon
D.N. Shimshon 9978500
ISRAEL
Email: <email>arnold@@skeeve.com</email>
URL: <ulink url="http://www.skeeve.com/">http://www.skeeve.com/</ulink></literallayout>

<literallayout class="normal">Copyright &copy; 2007, 2008, 2011, 2015, 2016, 2018, 2019, 2020, 2021, 2022
Arnold David Robbins
All Rights Reserved.</literallayout>
@end docbook

@ifnotdocbook
Copyright @copyright{} 2007, 2008, 2011, 2015, 2016, 2018, 2019, 2020, 2021, 2022 @*
Arnold David Robbins @*
All Rights Reserved.
@end ifnotdocbook
@sp 1
The @command{sortmail} program is copyright
@copyright{} 2007, 2008, 2011, 2015, 2016, 2018, 2019, 2020, 2021, 2022 by Arnold David Robbins.
It is published under
the conditions of the GNU General Public License, version 3.
@sp 1
As of Edition 1.1, the explanatory text provided in this document is
also published under the conditions of the GNU General Public License,
version 3.
@sp 2
This is Edition @value{EDITION} of @cite{@value{TITLE}}.
@end copying

@c Uncomment this for the release.  Leaving it off saves paper
@c during editing and review.
@c @setchapternewpage odd

@c Uncomment this if it's ever printed as a real book(let).
@c @shorttitlepage @value{SHORTTITLE}

@titlepage
@title @value{TITLE}
@subtitle @value{UPDATE-MONTH}
@author Arnold David Robbins

@ifnotdocbook
@c Include the Distribution inside the titlepage environment so
@c that headings are turned off.  Headings on and off do not work.

@page
@vskip 0pt plus 1filll
Published by:
@sp 1
Arnold David Robbins @*
P.O.B. 354 @*
Nof Ayalon @*
D.N. Shimshon 9978500 @*
ISRAEL @*
Email: @EMAIL{arnold@@skeeve.com,arnold AT skeeve.com} @*
URL: @url{http://www.skeeve.com/} @*

@insertcopying
@end ifnotdocbook
@end titlepage

@set DRAFT @i{DRAFT}

@ignore
@iftex
@headings off
@evenheading @thispage @| @value{DRAFT} @| @strong{@value{SHORTTITLE}}
@oddheading  @strong{@thischapter} @| @value{DRAFT} @| @thispage
@end iftex
@end ignore

@ifnottex
@ifnotdocbook
@ifnotxml
@node Top
@top General Introduction
@c Preface node should come right after the Top
@c node, in `unnumbered' sections, then the introductory chapter.
@c Licensing nodes are appendices, they're not central to TexiWebJr.

This file documents @code{sortmail}, a program that sorts Unix
mailbox files by thread.

@insertcopying
@end ifnotxml
@end ifnotdocbook
@end ifnottex

@menu
* Preface::                     Initial words.
* Introduction::                General introduction.
* Main Program::                The main program.
* Support Functions::           Supporting functions.
* Code Chunk Summaries::        Summary lists and cross-references.
* Concept Index::               The index.

@detailmenu
* Audience::                    Who should read this @value{DOCUMENT}.
* Conventions::                 Typographical conventions.
* Initialization::              Initializing data.
* Command line::                Processing command line options.
* Dumping thread summaries::    Dumping a thread summary.
* First item is first::         Treating the first message specially.
* Collecting Lines::            Collecting the input lines.
* Preparing To Sort::           Building the data structures.
* Collecting headers::          Building the arrays for the headers.
* Collecting the subject::      Collecting the @samp{Subject:} line.
* Collecting the message-id::   Collecting the @samp{Message-ID:} line.
* Mapping the data::            Creating the relationships.
* Sorting::                     Sorting them.
* Dumping::                     Writing out the sorted data.
* Summary of each variable::    Remembering what each array holds.
* Dumping the data itself::     Actually dumping the data.
* Debugging Code::              Debugging code all in one place.
* Compute date::                Compute the date.
* Month days::                  Return the number of days in a given month.
* Canonicalize subject::        Canonicalize the @samp{Subject:} line.
* Decode headers::              Decode base64 and quoted-printable encodings.
* Encodings::                   What encoded lines look like.
* Decoding Base64::             Decoding Base64 encoded lines.
* Decoding QP::                 Decoding Quoted-Printable encoded lines.
* File Definitions::            Source files by definition.
* Code Chunk Definitions::      Code chunks by definition.
* Code Chunk References::       Code chunks by reference.
@end detailmenu
@end menu

@c @summarycontents
@contents

@c Add these to the menu if they ever get included.
@c @node Foreword
@c @unnumbered Foreword

@node Preface
@unnumbered Preface

When I discovered a bug in my script for sorting Unix mailboxes,
I decided to rewrite it using TexiWeb Jr., my literate programming
system. It took a few hours, but now the bug is fixed.

A few months later I found that the program was much slower than
the original, and I was able to speed it back up.

@menu
* Audience::                    Who should read this @value{DOCUMENT}.
* Conventions::                 Typographical conventions.
@end menu

@node Audience
@unnumberedsec Intended Audience

If you're interested in @command{awk} programming with
@command{gawk} and want to see an interesting script, read
this @value{DOCUMENT}.

@node Conventions
@unnumberedsec Typographical Conventions

@c Copied mostly verbatim from the gawk manual.

@cindex Texinfo document formatting language
This @value{DOCUMENT} is written in an enhanced version of
@uref{http://www.gnu.org/software/texinfo/, Texinfo},
the GNU documentation formatting language.
A single Texinfo source file is used to produce both the printed and online
versions of a program's documentation.
@ifnotinfo
Because of this, the typographical conventions
are slightly different than in other books you may have read.
@end ifnotinfo

Examples you would type at the command-line are preceded by the common
shell primary and secondary prompts, @samp{$} and @samp{>}.  Input that
you type is shown @kbd{like this}.  Output from the command is preceded
by the glyph ``@print{}''.  This typically represents the command's
standard output.  Error messages, and other output on the command's
standard error, are preceded by the glyph ``@error{}''.  For example:

@example
$ @kbd{echo hi on stdout}
@print{} hi on stdout
$ @kbd{echo hello on stderr 1>&2}
@error{} hello on stderr
@end example

@ifnotinfo
In the text, command names appear in @code{this font}, while code segments
appear in the same font and quoted, @samp{like this}.  Options look
like this: @option{-f}.  Some things are emphasized @emph{like this},
and if a point needs to be made strongly, it is done @strong{like this}.
The first occurrence of a new term is usually its @dfn{definition} and
appears in the same font as the previous occurrence of ``definition''
in this sentence.  Finally, file names are indicated like this:
@file{/path/to/our/file}.
@end ifnotinfo


@node Introduction
@chapter Introduction

Internet mail is random. Much like IP packets,
email messages can be lost, duplicated, and arrive out of order.

I am a luddite. I still use Berkely Mail (in its modern incarnation of
@uref{http://heirloom.sourceforge.net/mailx.html, Heirloom @command{mailx}})
to read my email.  For many years I
wanted to write a script to sort my email by ``thread''---that is,
grouping related messages together by date and subject.

I wrote such a program in 2004, and it evolved in fits and starts until
about 2011.  In 2015, I found a serious bug in how @samp{Subject:} lines
were processed, so I decided to rewrite the program using TexiWeb Jr.

@node Main Program
@chapter Sorting A Mailbox

The structure of @file{sortmail.awk} follows that of most
@command{awk} programs: Initialization, collection and
maybe some processing, final processing in the @code{END}
rule, and support functions.

@menu
* Initialization::              Initializing data.
* Command line::                Processing command line options.
* Collecting Lines::            Collecting the input lines.
* Preparing To Sort::           Building the data structures.
* Sorting::                     Sorting them.
* Dumping::                     Writing out the sorted data.
* Debugging Code::              Debugging code all in one place.
@end menu

@post_create sortmail.awk chmod +x sortmail.awk
@(sortmail.awk@)=
@<Leading comments and license@>

BEGIN {
	@<Initialize variables and arrays@>
	@<Process options@>
}

@<Collect mailbox lines, build @code{Header} and @code{Body} arrays@>

END {
	@<Save last message's body@>
	@<Set up data for sorting@>
	@<Sort the data@>
	@<Dump the data@>
	@<Close thread summary file@>
}

@<Support Functions@>
@

@use_smallexample
@<Leading comments and license@>=
#! /usr/bin/gawk -f

# sortmail.awk --- sort a Unix style mailbox by "thread", in date+subject order.
# Use Message-ID header to detect and remove duplicates.  Requires GNU Awk for
# time/date and sorting functions but could be made to run on a POSIX awk
# with some work.
#
# Copyright (C) 2007, 2008, 2011, 2015, 2016, 2018, 2019, 2020, 2021
# Arnold David Robbins
# arnold@skeeve.com
#
# Sortmail.awk is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 3 of the License, or
# (at your option) any later version.
# 
# Sortmail.awk is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA
@
@use_example

@node Initialization
@section Initialization

Initialization is pretty simple.

@<Initialize variables and arrays@>=
@<Initialize constants@>
@<Initialize @code{Months}@>
@<Initialize @code{MonthDays}@>
@<Initialize other variables@>
@

What program could survive without @code{TRUE} and @code{FALSE}?

@<Initialize constants@>=
TRUE = 1
FALSE = 0
@

The @code{Months} array holds the names of the months. The
@code{MonthDays} array holds the number of days in each month.
They are used for parsing the @samp{Date:} header.
Setting up @code{Months} is straightforward using @code{split()}. This
is a typical idiom to use a bunch of strings as array indices.

@<Initialize @code{Months}@>=
split("Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec", months, " ")
for (i in months)
	Month[months[i]] = i	# map name to number
@

Finally, here are the other variables.
Each one's purpose will become clear as we progress.

@<Initialize other variables@>=
In_header = FALSE

# These keep --lint happier
Debug = 0
MessageNum = 0
Duplicates = 0

body = ""
@

@node Command line
@section Command Line Options

Over time, the program has started to acrete command-line
options. Here we process them.

@<Process options@>=
for (i = 1; i < ARGC; i++) {
	@<Thread summary option@>
	@<First item remains first option@>
}
@

@menu
* Dumping thread summaries::    Dumping a thread summary.
* First item is first::         Treating the first message specially.
@end menu

@node Dumping thread summaries
@subsection Dumping a thread summary

When I get really behind on my mail, I start to want a list of the threads
in my inbox.  To allow for this, we add a @option{-T @var{file}} option.
Using @option{-T} avoids conflicts with any (currently) existing @command{gawk}
option.

@<Thread summary option@>=
if (ARGV[i] == "-T" && ((i+1) in ARGV)) {
	Thread_summary_file = ARGV[i+1]
	delete ARGV[i]
	delete ARGV[i+1]
	++i		# skip over file name in the loop
}
@

We need the @code{Thread_summary_file} variable:

@<Initialize other variables@>=
Thread_summary_file = ""
@

The @code{dump_summary()} function dumps the thread summary.
It splits the information for the first message in each thread
apart, formats it nicely, and prints it.

@<Support Functions@>=
function dump_summary(messagenum, from, thread_info,
					  t, n, subj)
{
	# thread_info is in the form
	# <first date> SUBSEP <subject> SUBSEP <actual date> SUBSEP <message-id>
	sub(/^[Ff]rom:[[:space:]]*/, "", from)
	n = split(thread_info, t, SUBSEP)
	subj = t[2]
	if (subj != Last_summary_subject)
	{
		printf("%-5d \"%-25.25s\" %-33.33s %s\n",
			messagenum,
			subj,
			from,
			strftime("%Y-%m-%d", t[1])) > Thread_summary_file
		Last_summary_subject = subj
	}
}
@

We also need the @code{Last_summary_subject} variable:

@<Initialize other variables@>=
Last_summary_subject = ""
@

It's good practice to clean things up at the end.

@<Close thread summary file@>=
if (Thread_summary_file != "")
	close(Thread_summary_file)
@

@node First item is first
@subsection Keeping The First Item As First

In the case of a mailbox which stores mail until downloaded via IMAP,
the first item is special, and has to remain as the first one in the
mailbox, even if it's not the oldest. The initial message looks
something like this:

@smallexample
From MAILER-DAEMON Thu Jan  4 11:30:38 2018
Date: 04 Jan 2018 11:35:38 -0700
From: Mail System Internal Data <MAILER-DAEMON@@frenzy.freefriends.org>
Subject: DON'T DELETE THIS MESSAGE -- FOLDER INTERNAL DATA
Message-ID: <1515090938@@frenzy.freefriends.org>
X-IMAP: 1490643014 0000005528
Status: RO

This text is part of the internal format of your mail folder, and is not
a real message.  It is created automatically by the mail system software.
If deleted, important folder data will be lost, and it will be re-created
with the data reset to initial values.
@end smallexample

The option is @option{-k} (``keep'') to avoid conflict with
any (currently) existing @command{gawk} option.

@<First item remains first option@>=
if (ARGV[i] == "-k") {
	Keep_first = TRUE
	delete ARGV[i]
}
@

@<Initialize other variables@>=
Keep_first = FALSE
@


@node Collecting Lines
@section Collecting Lines

Previous versions of this program built up the data arrays as the input
mailbox was read.  In particular, each header was found and stored.
The problem is that @uref{https://www.ietf.org/rfc/rfc2822.txt, RFC 2822}
mail headers allow for continuation across lines by starting the second
and subsequent lines with whitespace.  Thus, you can't tell that a header
is continued without looking ahead.

The previous versions did not do this at all for the @samp{Subject:}
header, and they did not do it correctly for the @samp{Message-ID:}
header.

Thus, this version simply collects the header and body of each message,
and processes them at the end.

Each message in a Unix mailbox starts with a @samp{@w{From@ }} line.
Lines in the body that start with those five characters (F, r, o, m,
space) are escaped. When such a line is seen, that signals the start of
a new message and the collection of the header.

The header is separated from the body by an empty line; the body
continues to the next @samp{@w{From@ }} line. It is convenient to store
the terminating blank line as the last line of the header instead of as
the first line of the body.

It turns out to be faster to use @command{gawk}'s multidimensional
arrays than standard @command{awk}'s simulated multidimensional
subscripts.

Additionally, it turns out to be @emph{much} faster to collect
the body of each message as a single string using string concatenation
than to store each line of each body's message as a separate array
element.

If we are treating the first message specially, collect it
and store it off in separate variables.

@<Collect mailbox lines, build @code{Header} and @code{Body} arrays@>=
NR == 1 && Keep_first {
	First_message = $0
	while ((getline Line) > 0) {
		if (Line ~ /^From /) {
			$0 = Line
			break	#	fall into next rule
		}
		First_message = First_message "\n" Line
	}
}

{
	Line = $0
	if (Line ~ /^From /) {
		In_header = TRUE
		MessageNum++
		header_line = 1
		Header[MessageNum][header_line++] = Line
		if (MessageNum > 1) {
			Body[MessageNum-1] = body
		}
		body_line = 0
		body = ""
	} else if (In_header) {
		if (Line ~ /^$/) {
			In_header = FALSE
		}
		Header[MessageNum][header_line++] = Line
	} else {
		if (body_line == 0)
			body = Line
		else
			body = body "\n" Line
		body_line++
	}
}
@

In the @code{END} rule, we have to save the last message's body.

@<Save last message's body@>=
Body[MessageNum] = body
@

@node Preparing To Sort
@section Preparing For Sorting

Once all the lines have been collected, it's time to collect the headers
for each message. Of interest are the @samp{Date:}, @samp{Subject:},
and @code{Message-ID:} headers.

The subject identifies the thread, and the date associated with each
message gives us the ordering within the thread.  The date is turned
into a ``seconds since the epoch'' timestamp to use in sorting.

The first step is to get lists of subjects, dates, and message IDs, into
individual arrays, indexed by message number.  In this case, in the rest
of the code in this @value{SECTION}, the simple variable @code{i} is the
current message number.  (In retrospect, using a name like @code{msgnum}
would have been better.)

@<Set up data for sorting@>=
for (i = 1; i <= MessageNum; i++) {
	@<Collect headers@>
	@<Map subject, date, and message-id to index into text@>
}
@

@menu
* Collecting headers::          Building the arrays for the headers.
* Mapping the data::            Creating the relationships.
@end menu

@node Collecting headers
@subsection Collecting Header Information

RFC 2822 headers are case-independent.  Thus, I really should
use @code{tolower()} or @code{toupper()} before doing a regexp match. In
practice, almost always the first letter is uppercase and the the rest
are lowercase.  The message ID is an exception; different cases for the
@samp{ID} part are common.

The date is handled by the @code{compute_date()} function shown
in @ref{Compute date}.

@<Collect headers@>=
for (j = 1; j in Header[i]; j++) {
	if (Header[i][j] ~ /^[Dd]ate: /) {
		Date[i] = compute_date(Header[i][j])
	} else if (Header[i][j] ~ /^[Ss]ubject: /) {
		@<Process the @code{Subject:} line@>
	} else if (Header[i][j] ~ /^[Mm]essage-[Ii][Dd]: */) {
		@<Process the @code{Message-ID:} header@>
	}
}
@

@menu
* Collecting the subject::      Collecting the @samp{Subject:} line.
* Collecting the message-id::   Collecting the @samp{Message-ID:} line.
@end menu

@node Collecting the subject
@subsubsection Collecting The @samp{Subject:} Line

The @samp{Subject:} line needs care. If it's continued, it must
be collected fully. This is done with a loop, building up the full
line in @code{subj_line}.

There is an additional wrinkle. Header lines can be encoded in
different character encodings besides ASCII.  When encoded,
they may use either Base64 encoding
(see @url{https://tools.ietf.org/html/rfc3548, RFC 3548})
or a modified version of Quoted-Printable encoding
(see @url{https://en.wikipedia.org/wiki/Quoted-printable, the Wikipedia article}).
We have to decode these lines
before comparing them; we've seen cases where different messages
in the same thread came in with both encoded and unencoded @samp{Subject:}
lines!  The decoding is done by the @code{decode()} function
(@pxref{Encodings}).

The @code{canonacalize_subject()}
function (@pxref{Canonicalize subject}) converts the collected
line into a ``canonical'' form so that it can be used as a sort key:

@<Process the @code{Subject:} line@>=
subj_line = decode(Header[i][j])
for (k = j + 1; k in Header[i] && Header[i][k] ~ /^[[:space:]]/; k++)
	subj_line = subj_line "\n" decode(Header[i][k])
Subject[i] = canonacalize_subject(subj_line)
@

@node Collecting the message-id
@subsubsection Collecting The @samp{Message-ID:} Line

The @code{Message-ID:} header helps us identify duplicate
messages in the input mailbox.

@<Process the @code{Message-ID:} header@>=
@<Check for continued @code{Message-ID:} line, handle it@>
line = tolower(message_id_line)
split(line, linefields)

message_id = linefields[2]
Mesg_ID[i] = message_id	# needed for disambiguating message
@<Check for duplicates and print a message@>
@

The @samp{Message-ID:} header might be continued,
so we have to handle that. If it's continued, there
may or may not be trailing whitespace after the colon,
so the regexp constant used to test it should
include an optional space.  (This was a bug in previous versions of
the program.)

@<Check for continued @code{Message-ID:} line, handle it@>=
message_id_line = Header[i][j]
if (tolower(message_id_line) ~ /^message-id: *$/) {
	# line is only the header name, get the next line
	message_id_line = message_id_line " " Header[i][j+1]
}
@

Remember that @code{i} is the current message being processed.
The @code{Message_IDs} array tracks message numbers, and if
there are duplicates, all of the ones that are duplicated.

@code{Duplicates} becomes nonzero when there are duplicates.

@<Check for duplicates and print a message@>=
if (message_id in Message_IDs) {
	printf("Message %d is duplicate of %s (%s)\n",
		i, Message_IDs[message_id],
		message_id) > "/dev/stderr"
	Message_IDs[message_id] = (Message_IDs[message_id] ", " i)
	Duplicates++
} else {
	Message_IDs[message_id] = i ""
}
@

@node Mapping the data
@subsection Map The Keys Into An Index Into The Text

We use a triplet of (subject, date, message ID) to uniquely
identify each message. Standard @command{awk}'s multidimensional
arrays (creating a unique subscript by concatenating the
values separated by @code{SUBSEP}) work nicely here.
This data is tracked in the @code{SubjectDateId} array.

@<Map subject, date, and message-id to index into text@>=
@<Debug: Check for same triplet already seen@>

SubjectDateId[Subject[i], Date[i], Mesg_ID[i]] = i

@<Debug: Print current info@>
@<Build mapping of subject to earliest date for that subject@>
@

As a nice side-effect of correctly collecting the headers, duplicate
messages are automatically discarded. How?  Consider: Duplicate messages
will have the same date, subject, and message ID. As a result, the
assignment statement above stores the message number of just the last
duplicate.

Separately, we find the first date in the thread, which
lets us sort the threads.

@<Build mapping of subject to earliest date for that subject@>=
if (! (Subject[i] in FirstDates) || FirstDates[Subject[i]] > Date[i])
	FirstDates[Subject[i]] = Date[i]
@

@node Sorting
@section Sorting The Data

Sorting the data is simple: prepare the data and then sort it.
There's a lot of debugging code that probably isn't really needed anymore.

@<Sort the data@>=
@<Debug: Sorting: Print statistics@>
@<Create @code{Thread} array to sort by thread@>
@<Sort @code{Thread}, placing the results in @code{SortedThread}@>
@<Debug: Sorting: Print info on sorted threads@>
@<Make sure nothing is weird@>
@<Debug: Sorting: Dump info to @file{DUMP1}@>
@

We sort the messages by thread, which is essentially
the subject. The date the message was sent provides ordering
within the thread, and adding in the message-ID gives us
unique messages.

One issue remains: how to order each thread? The answer is
to order them by the earliest date associated with a subject,
thus ordering the threads by when they were started.  (Or,
more technically, by the earliest extant message in the thread.)

@<Create @code{Thread} array to sort by thread@>=
# Subscript is earliest date, subject, actual date, message-id
# Value is subject, actual date, message-id
for (i in SubjectDateId) {
	n = split(i, t, SUBSEP)
	if (n != 3) {
		printf("yowsa! n != 3 (n == %d)\n", n) > "/dev/stderr"
		exit 1
	}
	# now have subject, date, message-id in t
	# create index into Text
	Thread[FirstDates[t[1]], i] = SubjectDateId[i]
}
@

Once the @code{Thread} array is set up, with the indices
identifying both the threads and the earliest occurrence of
each, it's time to sort the data.  @command{gawk}'s built-in
@code{asorti()} function does this for us:

@<Sort @code{Thread}, placing the results in @code{SortedThread}@>=
n = asorti(Thread, SortedThread)	# Shazzam!
@

Finally, we double check that everything is as it should be.
This too could probably be removed.

@<Make sure nothing is weird@>=
if (n != MessageNum && Duplicates == 0) {
	printf("yowsa! n != MessageNum (n == %d, MessageNum == %d)\n",
		n, MessageNum) > "/dev/stderr"
#	exit 1
}
@

@node Dumping
@section Dumping The Data

Dumping the data is easy.
Go through each thread in order of sorted thread,
dumping the header and the body.
We make provision for writing out a summary list of
all threads.

@menu
* Summary of each variable::    Remembering what each array holds.
* Dumping the data itself::     Actually dumping the data.
@end menu

@node Summary of each variable
@subsection Reviewing our data

Let's review what each array is used for:

@multitable @columnfractions .50 .50
@headitem Array @tab Meaning
@item @code{Subject[@var{i}] =} @tab @var{i}'th @samp{Subject:} line
@item @code{Date[@var{i}] =} @tab @var{i}'th @samp{Date:} line
@item @code{Id[@var{i}] =} @tab @var{i}'th @samp{Message-Id:} line
@item @code{SubjectDateId[@var{subject}, @var{date}, @var{id}] =} @tab @var{i}; Map from headers to index
@item @code{FirstDates[@var{subject}] =} @tab Start date for thread
@item @code{Thread[@var{first date}, @var{subject}, @var{date}, @var{id}] =} @tab Index of message with subject, actual date, message id
@end multitable

@ignore
Subject[i]	i'th Subject line
Date[i]		i'th Date line
Id[i]		i'th MessageId line

SubjectDateId[<sub>, <date>, <id>] = i		from headers to index
FirstDates[<sub>]	start date for thread

Thread[earliest date, subject, actual date, message-id] =
@end ignore

@node Dumping the data itself
@subsection Actually Dumping the Data
When done, the output file should be identical in
size to the input (unless duplicates were removed),
with the same number of messages.

@<Dump the data@>=
if (Keep_first)
	print First_message

for (i = 1; i <= MessageNum; i++) {
	k = Thread[SortedThread[i]]
	for (j = 1; k in Header && j in Header[k]; j++) {
		print Header[k][j]
		if (Thread_summary_file && Header[k][j] ~ /^[Ff]rom: /)
			dump_summary(i, Header[k][j], SortedThread[i])
	}
	print Body[k]
}
@

Previous versions sent the output to a file with a fixed
name.  That was a flaw. This version corrects it, returning to
basic @cite{Software Tools} principles, and prints the results
to standard output.

The check @samp{k in Header} before the check @samp{j in Header[k]}
is important.  If a duplicate was removed, all but the last instance
of the duplicated message will not have their message numbers stored
in @code{Header}.  Without this check, when @code{Header[k]} is
checked, it first automatically springs into existence, but @emph{as a
scalar}. @command{gawk} then dies with a fatal message about attempting
to use a scalar as an array.

@node Debugging Code
@section Debugging Print Statements

The debugging statements are collected here so that they don't
disturb the main flow of the program. This is one of the benefits
of using literate programming tools.

The @code{Debug} variable can contain anything, but if we test for
words or patterns, it allows finer grained control over debugging
output. The technique is only used once in this program but was
quite effective when working on TexiWebJr itself.


@use_smallexample
@<Debug: Check for same triplet already seen@>=
if (Debug && (Subject[i], Date[i], Mesg_ID[i]) in SubjectDateId) {
	printf(\
("Message %d: Subject <%s> Date <%s> Message-ID <%s> already in" \
" SubjectDateId (Message %d, s: <%s>, d <%s> i <%s>)!\n"),
	i, Subject[i], Date[i], Mesg_ID[i],
	SubjectDateId[Subject[i], Date[i], Mesg_ID[i]],
	Subject[SubjectDateId[Subject[i], Date[i], Mesg_ID[i]]],
	Date[SubjectDateId[Subject[i], Date[i], Mesg_ID[i]]],
	Mesg_ID[SubjectDateId[Subject[i], Date[i], Mesg_ID[i]]]) \
		> "/dev/stderr"
}
@

@<Debug: Print current info@>=
if (Debug) {
	printf("\tMessage Num = %d, length(SubjectDateId) = %d\n",
		i, length(SubjectDateId)) > "/dev/stderr"
	if (i != length(SubjectDateId) && ! Printed1) {
		Printed1++
		printf("---> Message %d <---\n", i) > "/dev/stderr"
	}
}
@

@<Debug: Sorting: Print statistics@>=
if (Debug) {
	printf("length(SubjectDateId) = %d, length(Subject) = %d, length(Date) = %d\n",
		length(SubjectDateId), length(Subject), length(Date)) > "/dev/stderr"
	printf("length(FirstDates) = %d\n", length(FirstDates)) > "/dev/stderr"
}
@

@<Debug: Sorting: Print info on sorted threads@>=
if (Debug) {
	printf("length(Thread) = %d, length(SortedThread) = %d\n",
		length(Thread), length(SortedThread)) > "/dev/stderr"
}
@

@<Debug: Sorting: Dump info to @file{DUMP1}@>=
if (Debug) {
	for (i = 1; i <= n; i++)
		printf("SortedThread[%d] = %s, Thread[SortedThread[%d]] = %d\n",
	 		i, SortedThread[i], i, Thread[SortedThread[i]]) > "DUMP1"
	close("DUMP1")
	if (Debug ~ /exit/)
		exit 0
}
@
@use_example

@node Support Functions
@chapter Support Functions

@command{sortmail} needs some auxiliary functions.

@menu
* Compute date::                Compute the date.
* Month days::                  Return the number of days in a given month.
* Canonicalize subject::        Canonicalize the @samp{Subject:} line.
* Decode headers::              Decode base64 and quoted-printable encodings.
@end menu

@<Support Functions@>=
@<Turn the date into a timestamp@>
@<Compute the days in the given month for the given year@>
@<Canonicalize subject line@>
@<Decode encoded headings@>
@

@node Compute date
@section Computing The Date As A Timestamp

The @samp{Date:} header needs to be turned into a timestamp
of the ``seconds since the epoch'' form.  This function does the
job.

@<Turn the date into a timestamp@>=
# compute_date --- pull apart a date string and convert to timestamp

function compute_date(date_rec,		fields, year, month, day,
					hour, min, sec, tzoff, tzmin, timestamp)
{
	@<Split the record into fields@>
	@<Parse the @code{Date:} header@>
	@<Compute timezone offset@>
	@<Add timezone offset to hour@>
	@<If moved into next day, reset other values@>

	# -1 means DST unknown
	timestamp = mktime(sprintf("%d %d %d %d %d %d -1",
				year, month, day, hour, min, sec))

	# timestamps can be 9 or 10 digits.
	# canonicalize them into 11 digits with leading zeros
	return sprintf("%011d", timestamp)
}
@

The record is split into fields on spaces, colons, and commas:

@<Split the record into fields@>=
split(date_rec, fields, "[:, ]+")
@

We must deal with several different forms of @samp{Date:}
header. This is straightforward but a little tedious:

@<Parse the @code{Date:} header@>=
if (fields[2] ~ /Sun|Mon|Tue|Wed|Thu|Fri|Sat/) {
	if ($6 == "at") {
		# Date: Thu, Apr 26, 2012 at 7:04 AM
		year = fields[5] + 0
		month = Month[fields[3]]
		day = fields[4] + 0
		hour = fields[7] + 0
		if (tolower(fields[9]) == "pm")
			hour += 12
		min = fields[8] + 0
		sec = 0
		tzoff = 0
	} else if (fields[3] in Month) {
		# Date: Wed May 11 16:52:19 BST 2022  [ gag... ]
		year = fields[9] + 0
		month = Month[fields[3]]
		day = fields[4] + 0
		hour = fields[5] + 0
		min = fields[6] + 0
		sec = fields[7] + 0
		# Ugly special case...
		if (tzoff == "BST") {
			tzoff = "+0100"
			tzoff += 0
		} else
			tzoff = fields[8] + 0
	} else {
		# Date: Thu, 05 Jan 2006 17:11:26 -0500
		year = fields[5] + 0
		month = Month[fields[4]]
		day = fields[3] + 0
		hour = fields[6] + 0
		min = fields[7] + 0
		sec = fields[8] + 0
		tzoff = fields[9] + 0
	}
} else {
	# Date: 05 Jan 2006 17:11:26 -0500
	year = fields[4] + 0
	month = Month[fields[3]]
	day = fields[2] + 0
	hour = fields[5] + 0
	min = fields[6] + 0
	sec = fields[7] + 0
	tzoff = fields[8] + 0
}

# Date: 14 Oct 20 16:58:26 --- yes really! 2020 comes to us as 20! Grrrr
if (year < 100)
	year += 2000
@

Turn the timezone offset into a number.
Apparently only @samp{GMT} needs to be special cased.

@<Compute timezone offset@>=
if (tolower(tzoff) == "gmt")
	tzoff = 0
@<Handle a timezone offset with hours and minutes@>
@

Timezone values need not be in round values of hours,
they can also be partial. It's possible that adjusting
for the timezone offset can shift a timestamp backwards
across midnight, effectively changing the date:

@<Handle a timezone offset with hours and minutes@>=
# tzoff is usually of form -0200 or +0500 but
# can sometimes be of form +0530, so deal with that.
tzmin = 0
if (tzoff !~ /00$/) {
	# there are minutes in the tz offset
	tzmin = substr(tzmin, length(tzmin) - 2) + 0
	if (min - tzmin < 0) {
		min = 60 + (min - tzmin)
		if (--hour < 0) {
			hour = 23
			if (--day == 0) {
				day = 1
				if (--month == 0) {
					month = 1
					year--
				}
			}
		}
	} else
		min -= tzmin
}
@<Convert offset to hours@>
@

Finally, we must convert the offset into hours, and then change the sign
so that adding it will bring the time into GMT.  (Times west of GMT are
negative, so the sign must switch to positive, and vice versa.)

@<Convert offset to hours@>=
tzoff = int(tzoff / 100)
tzoff = -tzoff
@

We could try to compensate for the local timezone, but
this isn't really necessary. Why?
@code{mktime()} wants a local time, which can be
computed as:

@example
hour + tzoff @result{} GMT
GMT + LocalOffset @result{} local time
@end example

@noindent
But since all we need is distinct values for sorting, we just do everything in GMT:

@<Add timezone offset to hour@>=
hour += tzoff
@

Finally, after adding the timezone offset to the hour, we have to check if
we've moved into the next day, and adjust the other values appropriately:

@<If moved into next day, reset other values@>=
if (hour > 23) {
	hour %= 24
	day++
	if (day > days_in_month(month, year)) {
		day = 1
		month++
		if (month > 12) {
			month = 1
			year++
		}
	}
}
@

@node Month days
@section Computing The Number of Days In A Given Month

The @code{MonthDays} array helps in computing the correct
value:

@<Initialize @code{MonthDays}@>=
split("31 28 31 30 31 30 31 31 30 31 30 31", MonthDays, " ")
@

The number of days in a month is constant, except for February, where
the count varies if the year is a leap year or not. This function does
the work:

@<Compute the days in the given month for the given year@>=
# days_in_month --- how many days in the given month

function days_in_month(month, year)
{
	if (month != 2)
		return MonthDays[month]

	if (year % 4 == 0 && year % 400 != 0)
		return 29

	return 28
}
@

@node Canonicalize subject
@section Canonicalizing The @samp{Subject:} Line

The @samp{Subject:} line is one of the sort keys.  Mailers often mangle
it, so it must be converted to a ``canonical'' form for use as a sort key
and array subscript. This function does the job.  The comments alongside
each line of code explain what that line is doing:

@use_smallexample
@<Canonicalize subject line@>=
# canonacalize_subject --- trim out "Re:", "fw:", "fwd:", white space

function canonacalize_subject(subj_line)
{
	subj_line = tolower(subj_line)			# lower case the line
	sub(/^subject: +/, "", subj_line)		# remove "subject:"
	# remove "re:" (sv: for sweden); we've even seen Re[2]:,
	# "aw" is apparently german
	sub(/^((re|sv|aw)(\[[0-9]+\])?: *)+/, "", subj_line)
	sub(/^((fwd?): *)+/, "", subj_line)		# remove "fw:" and "fwd:" (forward)
	sub(/\(fwd\)[:space:]*$/, "", subj_line)		# remove trailing "(fwd)" (forward)
	gsub(/\n[[:space:]]+/, " ", subj_line)	# merge multiple lines
	sub(/[[:space:]]+$/, "", subj_line)		# remove trailing whitespace
	gsub(/[[:space:]]+/, " ", subj_line)	# collapse multiple whitespace

	if (subj_line ~ /^[[:space:]]*$/)		# only Re:, Fw:, Fwd:, etc
		subj_line = "<<<EMPTY>>>"

	return subj_line						# return the result
}
@
@use_example

@node Decode headers
@section Decoding Base64 and Quoted-Printable Encodings

In this @value{SECTION} we will look at how to decode encoded headers.

@menu
* Encodings::                   What encoded lines look like.
* Decoding Base64::             Decoding Base64 encoded lines.
* Decoding QP::                 Decoding Quoted-Printable encoded lines.
@end menu

@node Encodings
@subsection What Encoded Lines Look Like

@c https://en.wikipedia.org/wiki/Quoted-printable
@c https://en.wikipedia.org/wiki/MIME#Encoded-Word

As part of updates to the Internet mail standards
(see @uref{https://tools.ietf.org/html/rfc2047, RFC 2047}),
it's possible for the data in
header lines to be encoded using 7-bit ASCII characters.
Such a line might look like this:

@smallexample
Subject: =?UTF-8?B?UmU6IElzIGEgRlNGIGNvcHlyaWdodCBhc3NpZ25tZW50IHJlcXVpcmVkIHRvIGNvbnRyaQ==?=
        =?UTF-8?B?YnV0ZSB0byBnYXdr?=
@end smallexample

In this example, the encoded value is delimited by @samp{=?}
and @samp{?=}.  The character set is @samp{UTF-8} (any known, standard
character set is allowed), and the encoding is Base64 (denoted by
the @code{B} between the question marks).
The Quoted-Printable encoding is also valid; it would
be denoted by a @code{Q}. Letter case is irrelevant in the character
set name, the encoding indicator, and in Base64 data.

This example shows a @samp{Subject:} line that is continued,
where the continuation is also encoded. We have to be able to handle
this case as well.

The @code{decode()} function decodes the encoded part of the
incoming line based on the encoding indicator, rebuilds the full
line, and returns the result. If there is no encoded data, it returns
the original line.

@<Decode encoded headings@>=
function decode(string,		pat_b, pat_q, full_pat, data, front, back)
{
	@<Extract front and back parts of the line and save for later@>
	@<Encoding Patterns@>

	if (string ~ pat_b) {
		data = gensub(pat_b, "\\1", 1, string)
		data = decode_base64(data)
		string = front data back
	} else if (string ~ pat_q) {
		data = gensub(pat_q, "\\1", 1, string)
		data = decode_quoted_printable(data)
		string = front data back
	}
	return string
}
@

This step saves the text around the encoded data, typically the
header name (@samp{Subject:}), and any trailing whitespace:

@<Extract front and back parts of the line and save for later@>=
full_pat = "(.*)=\\?[^?=]+\\?[BbQq]\\?.*\\?=(.*)"
front = gensub(full_pat, "\\1", 1, string)
back = gensub(full_pat, "\\2", 1, string)
@

These patterns let us detect which encoding is used and pull
out the actual encoded data with @code{gensub()}.
Base64 data comes in multiples of four characters (or should, anyway);
Quoted-Printable data doesn't have such a restriction.
The doubled backslashes are needed here to get a single backslash
into the string, in order to escape the @code{?}, which is regular
expression metacharacter:

@<Encoding Patterns@>=
pat_b = ".*=\\?[^?=]+\\?[Bb]\\?(.{4,})\\?=.*"
pat_q = ".*=\\?[^?=]+\\?[Qq]\\?(.*)\\?=.*"
@

@node Decoding Base64
@subsection Decoding Base64--encoded Lines

Since the whole program is written in @command{awk}, we need
an implementation of the Base64 decoding algorithm that is also
written in @command{awk}.

In the 21st century, we have the Internet; chances are good that
someone has already done this.  And indeed, after searching, we
found @uref{https://github.com/shane-kerr/AWK-base64decode,
this implementation}, which is described in detail in
@uref{https://dnshane.wordpress.com/2017/03/10/decoding-base64-in-awk/,
this blog post}.

The original code is portable to any version of @command{awk}.
After reviewing it some, I chose to modify the code, since it needs
to return the decoded string instead of printing it. While I was at it,
I chose to take advantage of features available in @command{gawk}.
Here's the result:

@<Decode encoded headings@>=
# The script implements Base64 decoding, based on RFC 3548:
#
# https://tools.ietf.org/html/rfc3548
#
# It is heavily modified from
# https://github.com/shane-kerr/AWK-base64decode
# See https://dnshane.wordpress.com/2017/03/10/decoding-base64-in-awk/
# for a description of the algorithm and the original code.

# create our lookup table
BEGIN {
	# Letters and digits
	lets = "ABCDEFGHIJKLMNOPQRSTUVWXYZ" \
	       "abcdefghijklmnopqrstuvwxyz0123456789"
	split(lets, l, "")
	for (i in l)
		BASE64[l[i]] = i - 1

	# and finally our two additional characters
	BASE64["+"] = 62
	BASE64["/"] = 63
	# also add in our padding character
	BASE64["="] = -1
}


function decode_base64(encoded,
						result, data, i, total, g0, g1, g2, g3) # locals
{
	result = ""

	total = split(encoded, data, "")
	for (i = 1; i + 3 <= total; i += 4) {
		g0 = BASE64[data[i + 0]]
		g1 = BASE64[data[i + 1]]
		g2 = BASE64[data[i + 2]]
		g3 = BASE64[data[i + 3]]

		check(g0, data[i + 0], i + 0)
		check(g1, data[i + 1], i + 1)
		check(g2, data[i + 2], i + 2)
		check(g3, data[i + 3], i + 3)

		result = result sprintf("%c", lshift(g0, 2) + rshift(g1, 4))
		if (g2 != -1) {
			result = result sprintf("%c", lshift(and(g1, 0xF), 4) + \
											rshift(g2, 2))
			if (g3 != -1) {
					result = result sprintf("%c",
										lshift(and(g2, 0x3), 6) + g3)
			}
		}
	}
	if (i < remaining) {
		printf("Extra characters at end of Base 64 encoded string:" \
				"\"%s\"\n",
				substr(encoded, i)) > "/dev/stderr"
		exit 1
	}

	return result
}
@

The @code{check()} function avoids code duplication. I have added
additional debugging information to the original code's error message.
The original code noted that using @code{exit} isn't necessarily right
for all situations. It remains here, but is probably not the best
thing to do.

@<Decode encoded headings@>=
function check(out, inc, pos)
{
	if (out == "") {
		printf("Unrecognized character %c (%c @ %d) " \
				"in Base 64 encoded string\n",
						out, inc, pos) > "/dev/stderr"
		exit 1
	}
}
@

The Base64 encoding uses printable characters to hold values between zero
and 63. Such values contain up to 6 bits of information. Thus four encoded
bytes hold three bytes of (possibly binary) data.  The bit-fiddling
restores the data, as shown in @ref{figure-decoded-bits}.
The numbers indicate which original encoded character
was the source of the given sets of bits. The doubled lines demarcate
the original values as well:

@float Figure,figure-decoded-bits
@caption{The bits after decoding}
@example
+-===========-====+========-========+====-===========-+
| 1 1 1 1 1 1 2 2 | 2 2 2 2 3 3 3 3 | 3 3 4 4 4 4 4 4 |
+-===========-====+========-========+====-===========-+
      Byte 1             Byte 2            Byte 3
@end example
@end float

@node Decoding QP
@subsection Decoding Quoted-Printable--encoded Lines

Quoted-Printable encoding leaves most characters alone, but encodes
characters that are not printable or not ASCII. The encoding is simple;
such encoded characters consist of an @samp{=} character followed by
two hexadecimal digits. For example:

@example
$ @kbd{printf 'hi\001there\n' | mimencode -q}
@print{} hi=01there
@end example

The first step in being able to decode these strings is to be
able to map hexadecimal digits to integers. This rule sets
up such a table in the @code{Hex} array. By using @code{toupper()}
we arrange that both uppercase and lowercase hexadecimal digits will
work correctly.

@<Decode encoded headings@>=
BEGIN {
	hexdigs = "0123456789abcdef"
	n = split(hexdigs, h, "")
	for (i = 1; i <= n; i++)
		Hex[toupper(h[i])] = hex[h[i]] = i - 1;

}
@

Decoding the data is relatively straightforward, given the extended
version of the @code{split()} function in @command{gawk}. We split
on occurrences of @samp{=} followed by two hexadecimal digits. The
values are stored in the @code{seps} array and the intervening text
in the @code{parts} array.

We can then reassemble the line, converting the encoded data to
a real character as we go:

@page
@<Decode encoded headings@>=
function decode_quoted_printable(data,
						n, i, parts, seps, result, converted) # locals
{
	if (index(data, "=") == 0)	# no encoded characters
		return data

	n = split(data, parts, /=[[:xdigit:]]{2}/, seps)
	result = seps[0]	# real or ""
	for (i = 1; i in parts; i++) {
		converted = ""
		if (i in seps)
			converted = sprintf("%c",
					Hex[substr(seps[i], 2, 1)] * 16 + \
					Hex[substr(seps[i], 3, 1)])

		result = result parts[i] converted
	}
	return result
}
@

@node Code Chunk Summaries
@appendix Code Chunk Summaries

This @value{APPENDIX} presents alphabetical lists of
all the file definitions, the code chunk definitions,
and the code chunk references.

@menu
* File Definitions::            Source files by definition.
* Code Chunk Definitions::      Code chunks by definition.
* Code Chunk References::       Code chunks by reference.
@end menu

@node File Definitions
@appendixsec Source File Definitions

@print_file_defs

@node Code Chunk Definitions
@appendixsec Code Chunk Definitions

@print_code_defs

@node Code Chunk References
@appendixsec Code Chunk References

@print_code_refs

@node Concept Index
@unnumbered Index

@printindex cp

@inlinefmt{docbook, <para></para>}

@bye

TODO:
